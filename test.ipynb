{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 動作テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "spark = SparkSession.builder.config('spark.sql.session.timeZone', 'UTC').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b      c\n",
       "10  1  100    one\n",
       "20  2  200    two\n",
       "30  3  300  three\n",
       "40  4  400   four\n",
       "50  5  500   five\n",
       "60  6  600    six"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(df)\n",
    "type(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|  a|  b|    c|\n",
      "+---+---+-----+\n",
      "|  1|100|  one|\n",
      "|  2|200|  two|\n",
      "|  3|300|three|\n",
      "|  4|400| four|\n",
      "|  5|500| five|\n",
      "|  6|600|  six|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+---+\n",
      "|  a|  b|    c|   d|  f|\n",
      "+---+---+-----+----+---+\n",
      "|  1|100|  one| 200| on|\n",
      "|  2|200|  two| 400| tw|\n",
      "|  3|300|three| 600| th|\n",
      "|  4|400| four| 800| fo|\n",
      "|  5|500| five|1000| fi|\n",
      "|  6|600|  six|1200| si|\n",
      "+---+---+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2 = sdf \\\n",
    "\t.withColumn(\"d\", sdf[\"b\"] * 2) \\\n",
    "\t.withColumn(\"f\", F.substring(\"c\", 0, 2))\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ファイル出力する際、内部的にパーティション分割されたデータのパーティション数にファイルが分かれて保存される\n",
    "# coalesceを使うことでパーティション数を減らすことができる\n",
    "# （ただし、そのための負荷はある。パーティションを減らした結果、1つのノードのメモリに収まらないデータ量になったらメモリエラー）\n",
    "# repartitionでもパーティション数をコントロールできるが、前者は減らす専用に最適化された関数\n",
    "sdf2.coalesce(1).write.parquet(\"test\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+---+\n",
      "|  a|  b|    c|   d|  f|\n",
      "+---+---+-----+----+---+\n",
      "|  1|100|  one| 200| on|\n",
      "|  2|200|  two| 400| tw|\n",
      "|  3|300|three| 600| th|\n",
      "|  4|400| four| 800| fo|\n",
      "|  5|500| five|1000| fi|\n",
      "|  6|600|  six|1200| si|\n",
      "+---+---+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2 = spark.read.parquet(\"test/*.parquet\")\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>one</td>\n",
       "      <td>200</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>two</td>\n",
       "      <td>400</td>\n",
       "      <td>tw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>three</td>\n",
       "      <td>600</td>\n",
       "      <td>th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>four</td>\n",
       "      <td>800</td>\n",
       "      <td>fo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>five</td>\n",
       "      <td>1000</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>six</td>\n",
       "      <td>1200</td>\n",
       "      <td>si</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b      c     d   f\n",
       "0  1  100    one   200  on\n",
       "1  2  200    two   400  tw\n",
       "2  3  300  three   600  th\n",
       "3  4  400   four   800  fo\n",
       "4  5  500   five  1000  fi\n",
       "5  6  600    six  1200  si"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sdf2.toPandas()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
